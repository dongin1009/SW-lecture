{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI&데이터 마이닝 7, 8일차 (2020.08.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 회귀와 분류\n",
    "## 예측 개념 소개\n",
    "### 예측??\n",
    "- 보이지 않는 미래의 데이터에 대해 출력값을 예측(회귀, regression)하거나 레이블을 예측(분류, classification)\n",
    "- 주요 전략\n",
    "    - 훈련 데이터 집합 수집\n",
    "    - 입력 독립변수(X)를 출력 종속변수(Y)로 매핑하는 함수 구축\n",
    "- 가정 1) X와 Y 사이에는 연관성이 있음 : Y는 독립변수 X에 의존적\n",
    "- 가정 2) 미래 데이터는 훈련 데이터 집합과 같은 분포를 가짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매핑(Mapping) 함수 = 가설(hypothesis) 함수\n",
    "- 선형 / 버선형 관계 모델링\n",
    "- 다수의 내부 파라미터 가지며 최적의 적합(fit, 훈련을 통한 모델 생성)을 위해서는 반드시 최적화 되어야 함\n",
    "- 최적화 전략 : 예측 알고리즘을 최소화 문제로 수식화\n",
    "    - 손실(loss) : 예측 실패를 수치화한 것\n",
    "    - 손실 함수(loss function) : 손실 계산을 위한 함수\n",
    "        - 수집한 데이터(X, Y)의 독립변수 X에 대한 예측 출력(Y_pred)과 Y를 비교\n",
    "    - 최적화 == 손실 최소화 ?\n",
    "        - 가장 일반적인 방법은 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수와 기울기 하강\n",
    "### 손실 함수(Loss Function)\n",
    "- 모델의 예측 손실을 계산하는 수학식\n",
    "- 예측 모델별로 서로 다른 함수 적용\n",
    "\n",
    "### 선형회귀(Linear Regression) 예\n",
    "y_pred = h(theta)(x) = (theta_1)x + (theta_0)\n",
    "- x : X의 한 레코드\n",
    "- h : hypothesis 함수\n",
    "- (theta_1), (theta_2) : 각각의 기울기와 절편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 하강 (Gradient Descent)\n",
    "- 손실을 최소화하기 위한 손실함수의 파라미터 최적화\n",
    "- 미분(derivative)이용\n",
    "    - 미분값이 음수이면 오른쪽으로 이동 -> theta를 크게\n",
    "    - 미분값이 양수이면 왼쪽으로 이동 -> theta를 작게\n",
    "    - learning rate : step size가 너무 크면 최솟값을 지나갈 수 있고, 너무 작으면 시간이 너무 많이 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 모델 품질\n",
    "- 훈련 데이터 집합에 대한 예측 정확도 및 테스트 데이터 집합(훈련에 사용되지 않은 데이터 집합)에 대한 예측 정확도를 측정\n",
    "- 고분산(High variance) : 과소적합(Underfit)\n",
    "- 고편향(High bias) : 과적합(Overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀(Regression)\n",
    "### 개요\n",
    "- 입력 데이터를 숫자값(예측값)으로 매핑\n",
    "- 집값, 멀리뛰기 거리, 홈런 숫자 등 예측 가능\n",
    "\n",
    "### 모델 예측 지표\n",
    "- MSE(Mean Squared Error) : 선형회귀의 손실함수<br>\n",
    "MSE = 1/m(시그마(y_pred - y_i)^2)\n",
    "- 큰 입력데이터에는 큰 MSE 값, 작은 입력데이터에는 작은 MSE 값 -> 복수 케이스에 대한 평가는 어려움\n",
    "    - 주택가격 예측 : 수억 ~ 수백억\n",
    "    - 멀리뛰기 예측 : 수십m\n",
    "- MSE 정규화\n",
    "    - 주택가격 예측 지표 0.8과 멀리뛰기 예측 지표 0.8은 같은 수준의 예측\n",
    "    - 결정 계수(Coefficient Determination)<br>\n",
    "    R^2 Score = 1 - (MSE/분산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_boston()\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_boston():\n",
    "    dataset = load_boston()\n",
    "    df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "    df['MEDV'] = dataset.target\n",
    "    df.index.name = 'record'\n",
    "    \n",
    "    # print(df.head())\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'MEDV'], df['MEDV'], test_size=.33, random_state=42)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is = 0.7261570836552478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_boston()\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('r2 score is = ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다변량 형식으로 확장\n",
    "### 개요\n",
    "- 대부분의 데이터 집합은 하나 이상의 입력 속성을 가짐\n",
    "- 가설 함수를 다수 변수로 확장할 필요 있음 -> 행렬 이용 <br>\n",
    "y_pred = h(theta)(x) = (theta_1)x + (theta_0)\n",
    "- X : 속성 행렬, (theta) : 파라미터 행렬(theta_0, theta_1, theta_2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 페널티 회귀\n",
    "### 개념\n",
    "- 전통적인 선형 회귀는 많은 가정을 전제로 하고 있음\n",
    "    - 독립 변수와 종속 변수가 선형 관계 -> Polynomial regression, Generalized Additive Model(GAM)\n",
    "    - 오차항의 확률분포가 정규분포 -> Generalized Linear Model(GLM)\n",
    "    - 오차항에 자기 상관성이 없음 -> Auto-regression\n",
    "    - 데이터에 아웃라이어가 없음 -> Robust regression, Quantile regression\n",
    "    - 독립변수 간에 상관성이 없음 -> Ridge regression, Lasso regression, Elastic Net regression, Principal Component REgression (PCR), Partial Least Square(PLS) regression\n",
    "- 다중 공신성\n",
    "    - 다변량 회귀분석시 독립변수들 간에 상관관계가 존재\n",
    "    - 독립변수들간의 상관관계가 강하면 각 독립변수의 설명력이 떨어져서 회귀 분석이 제대로 수행되지 않음\n",
    "    - 해결 방안\n",
    "        - 상관관계가 높은 독립변수 제거\n",
    "        - 손실함수에 페널티를 추가하여 파라미터 영향력 축소 : **Ridge regression, Lasso regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is = 0.7050894840749293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_boston()\n",
    "\n",
    "clf = Lasso(alpha=0.3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('r2 score is = ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is = 0.7241938555718039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_boston()\n",
    "\n",
    "clf = Ridge(alpha=0.3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('r2 score is = ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEDV와 다른 features가 상관관계가 높지 않아 r2 score가 낮게 나옴(Linear Regression과 비교)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류(Classification)\n",
    "### 개요\n",
    "- 입력 데이터를 카테고리 클래스 레이블로 예측\n",
    "- 참/거짓, 저위험/중위험/고위험, 동물의 종\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Linear SVM / RBF SVM\n",
    "\n",
    "### 분류 모델 예측 지표\n",
    "- 정확도 : 예측한 것 중 예측이 맞은 비율\n",
    "- 정밀도(Precision) : 예측한 것 중 True의 비율  TP / TP+FP\n",
    "- 재현율(Recall) : True라고 예측한 것이 실제로 True인 비율  TP / TP+FN\n",
    "- F1점수 : 정밀도와 재현율을 모두 고려  2 * ((Precision x Recall) / (Precision + Recall))\n",
    "- 예) 그룹A(정상, Negative) 90개, 그룹B(암환자, Positive) 10개\n",
    "    - 100번의 예측을 모두 A로 예측 : 90% 정확\n",
    "    - 100번의 예측을 모두 B로 예측 : 10% 정확\n",
    "\n",
    "### 복수 클래스 분류\n",
    "- 레이블이 3개 이상인 경우의 분류 방법\n",
    "- One vs. All, One vs One\n",
    "- One-vs-rest(be built = 4)\n",
    "    - [A] vs [B, C, D], [B] vs [A, C, D], [C] vs [A, B, D], [D] vs [A, B, C}\n",
    "    - 각 분류 모델은 모든 레이블의 데이터로 훈련\n",
    "    - 각 분류 모델로 예측을 수행 후 가장 점수가 높은 것 선택\n",
    "- One-vs-one(be built = 6)\n",
    "    - [A] vs [B], [A] vs [C], [A] vs [D], [B] vs [C], [B] vs [D], [C] vs [D]\n",
    "    - 각 분류 모델은 해당 레이블의 데이터로만 훈련\n",
    "    - 각 분류 모델로 예측을 수행하고 가장 많이 예측된 레이블을 선택\n",
    "    \n",
    "### 분류 예제 데이터 집합\n",
    "- make_moons : 초승달 모양 클러스터 두개의 형상\n",
    "- make_blobs : 동방성 가우시안 정규분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_moon_data():\n",
    "    X, y = make_moons(n_samples=150, noise=0.4, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### 개요\n",
    "- 회귀 기법이지만 실제로는 분류 기법\n",
    "- 가설함수로 시그모이드(sigmoid)함수를 사용\n",
    "\n",
    "### 가설함수\n",
    "- 분류를 위해서는 Z = (theta_1)x + (theta_0) 일 때 x에 대한 가설함수의 출력이 0 또는 1이 되도록 해야 함\n",
    "- Sigmoid 함수 이용 <br>\n",
    "y_pred = h(theta)(Z) = sigmoid(Z)\n",
    "\n",
    "### 손실함수 J\n",
    "- y가 1일때 h(theta)가 1에 가까운 값을 출력하면 J는 0에 가까워지고, (  loss = -log(h(theta))  )\n",
    "- y가 0일때 h(theta)가 0에 가까운 값을 출력하면 J가 1에 가까운 값을 갖도록 설계 (  loss = -log(1-h(theta))  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_moon_data()\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('f1 score is = ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Solver : 최적화 방법\n",
    "    - newton-cg, liblinear, sab, saga\n",
    "    - lbfgs : 가장 일반적, L2 penalty 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', penalty='l2', C=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('f1 score is = ' + str(f1_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(Support Vector Machine)\n",
    "### 개요\n",
    "- 로지스틱 회귀 모델의 확장\n",
    "- 두 클래스 데이터들 간의 최대 마진(margin)을 찾아내는 것이 목표\n",
    "- 선형 분류 뿐 아니라 커널 트릭(Kernel Trick)을 이용해 비선형 분류도 가능\n",
    "\n",
    "### 소프트 마진 (C)\n",
    "- 마진을 너무 strict 하게 유지하면 과적합 현상 발생\n",
    "- 일부 오 분류를 허용하는 마진을 찾도록 C 값을 조정하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_moon_data()\n",
    "\n",
    "clf = SVC(kernel='linear', C=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('f1 score is = ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커널 트릭(Kernel Trick)\n",
    "- 비선형 분류를 위해 데이터를 다른 차원으로 매핑하는 것\n",
    "- 가우시안 (Gaussian) 커널을 이용하여 gamma를 이용해 매핑 정도를 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.782608695652174\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma=2, C=1) # default = gaussian\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('f1 score is = ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트리 기반 분류 (Tree-based Classification)\n",
    "### 개요\n",
    "- 인간이 의사 결정하는 과정과 유사한 방식, 직관적\n",
    "- 특별한 처리 없이 다중 클래스에 대한 분류가 용이\n",
    "- 수치 데이터, 범주형 데이터 모두 처리가 가능하며 범주형 데이터는 one-hot encoding 등의 방법을 이용해서 전처리 필요\n",
    "\n",
    "### 주요 방법\n",
    "- 의사결정 트리 (Decision Tree)\n",
    "    - 직관적이며 사람이 분류과정을 이해하기 쉬움\n",
    "    - 트리가 복잡해지면 과적합 문제 발생\n",
    "- 랜덤 포레스트 (Random Forest)\n",
    "    - 여러 개의 작은 트리를 조합(ensemble)하여 분류\n",
    "    - 과적합 문제 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사 결정 트리 (Decision Tree)\n",
    "### 개요\n",
    "- 인간이 의사결정하는 방식과 유사\n",
    "- 루트 노드는 모든 데이터 포함하며 분류 기준에 따라 자식 노드로 분할됨\n",
    "### Gini 기반 노드 분할\n",
    "- 어떤 기준으로 어떻게 분할하는지가 가장 중요\n",
    "- 불순도(impurity) :\n",
    "    - 현재 노드의 예측 신뢰도, 얼마나 서로 다른 클래스의 데이터가 섞여 있는지를 나타내는 지표\n",
    "    - 불순도가 높으면 예측 신뢰도가 낮음\n",
    "- 노드의 분할은 자식 노드의 불순도 합이 최소가 되는 방향으로<br>\n",
    "location to split node = min(Imp_child1 + Imp_child2)\n",
    "- 불순도의 수치화 : Gini, Entropy\n",
    "- Gini 불순도\n",
    "    - j : 클래스의 수\n",
    "    - p_i : 어떤 데이터의 i번째 클래스에 속할 확률<br>\n",
    "    Gini imp = 1 - (1 to j sigma ( p_i)^2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.7391304347826089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_moon_data()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('f1 score is = ' + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트 (Random Forest)\n",
    "### 개요\n",
    "- 크기가 작고 과소적합된 다수의 트리가 ensemble 된 분류 기법\n",
    "- 튜닝이 크게 필요 없으며, 과적합 문제가 발생하지 않음\n",
    "- 다수 트리에 대해 분류를 시도하여 다수 분류결과에 대한 투표로 최종 분류 결과 확정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is = 0.7755102040816326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_moon_data()\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=4, n_estimators=4, max_features='sqrt', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('f1 score is = ' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
